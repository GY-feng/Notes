# 张量：

## 1，实际数据转为浮点数：

深度神经网络通常在不同阶段学习将数据从一种形式转换为另一种形式，这意味着每个阶段转换的数据可以被认为是一个中间表征序列。
这些中间表征是浮点数的集合，它们描述输入的特征，并以一种有助于描述输入映射到神经网络输出的方式捕获数据的结构。
浮点数的集合以及它们的操作是现代 AI 的核心。
![alt text](image.png)
中间表征是将输入与前一层神经元的权重结合的结果，每个中间表征对之前的输入都是唯一的
PyTorch 如何处理和存储数据：张量
在深度学习中，张量可以将向量和矩阵推广到任意维度
多维数组，张量的维度与用来表示张量中标量值的索引数量一致
![alt text](image-1.png)

## 2,张量：多维数组

张量是一个数组，也就是一种数据结构，它存储了一组数字，这些数字可以用一个索引单独访问，也可以用多个索引访问。
使用更有效的张量数据结构，从图像到时间序列等许多类型的数据，甚至是句子都可以表示出来。
通过定义张量上的操作，我们可以同时对数据进行高效的切片和操作

```py
import torch
a=torch.ones(3)#创建一个大小为3的一维张量，用1.0来填充
print(a[0])#我们可以使用从 0 开始的索引来访问一个元素，或者给它指定一个新值
print(a)
```

PyTorch 张量或 NumPy 数组通常是连续内存块的视图，这些内存块包含未装箱的 C 数字类型，而不是 Python 对象
![alt text](image-2.png)

```py
#构造函数的写法：
points=torch.tensor([4.0,1.0,5.0,3.0,2.0,1.0])
#向构造函数传递了一个元素为列表的列表
ppoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])
#查看张量的形状：
print(ppoints.shape)
#还可以使用 zeros()或 ones()函数来初始化张量，以元组的形式来指定大小
p=torch.zeros(3,2)
#用 2 个索引来访问张量中的单个元素
print(ppoints[0,1])
#得到第一个点的坐标：输出是另一个张量,引用了张量 points 中第 1 行的值。
#但在此过程中，没有分配了一个新的内存块！！！因为那样效率会很低
print(ppoints[0] )

```
## 3.3索引张量：

操作向量和操作列表有所相似：
```py
l=list(range(6))#列表中元素有0到5
print(l[:])#列表当中的所有元素
print(l[1:4])#包含第 1 个元素到第 3 个元素，不包含第 4 个元素
print(l[1:])#包含第 1 个元素到列表末尾的元素
print(l[:4])#从列表开始到第 3 个元素，不包含第 4 个元素
print(l[:-1])#从列表的开始到最后一个元素之前的所有元素
print(l[1:4:2])#从第 1 个元素（包含）到第 4 个元素（不包含），移动步长为 2

points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])
print(points[1:])#第 1 行之后的所有行，隐含所有列
print(points[1:,:])#第 1 行之后的所有行，所有列
print(points[1:,0])#第 1 行之后的所有行，第 1 列
print(points[None])#增加大小为 1 的维度，就像 unsqueeze()方法一样
```